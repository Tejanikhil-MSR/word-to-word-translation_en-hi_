{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b69ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\masab\\\\anaconda3\\\\envs\\\\sarvamai\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24a7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext-wheel\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fdcc300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(fasttext.FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b6bff19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['कृपया', 'प्रतीक्षा', 'करें']\n",
      "<class 'fasttext_pybind.loss_name'>\n"
     ]
    }
   ],
   "source": [
    "print(fasttext.FastText.tokenize(\"कृपया प्रतीक्षा करें\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f114cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cde1045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the content column of the pandas as follows :\n",
    "# Remove non hindi text from the text\n",
    "# Remove numericals \n",
    "# Remove extraspaces and \"\\n\" \"words enclosed in ()\", DoubleQuotes, \"special characters\"\n",
    "# Merge the paragraphs (or) points of an article to 1 paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8b746a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_hindi_text(text):\n",
    "    \"\"\"Preprocess text to keep only Hindi, remove unwanted elements, and merge into one paragraph\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Split text into sentences/segments (handling newlines and points)\n",
    "    segments = text.replace('\\n', ' ').split(' ')\n",
    "    \n",
    "    # Step 2: Keep only Hindi text\n",
    "    hindi_text = []\n",
    "    for segment in segments:\n",
    "        if segment.strip():\n",
    "            try:\n",
    "                if detect(segment) == 'hi':\n",
    "                    hindi_text.append(segment)\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    # Rejoin segments into a single string\n",
    "    text = ' '.join(hindi_text)\n",
    "    \n",
    "    # Step 3: Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Step 4: Remove text in parentheses\n",
    "    text = re.sub(r'\\([^()]*\\)', '', text)\n",
    "    \n",
    "    # Step 5: Remove double quotes\n",
    "    text = text.replace('\"', '')\n",
    "    \n",
    "    # Step 6: Remove special characters (keep Hindi characters and spaces)\n",
    "    # Hindi Unicode range: \\u0900-\\u097F\n",
    "    text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n",
    "    \n",
    "    # Step 7: Remove extra spaces and normalize\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_dataframe(csv_file, column_name='full_text'):\n",
    "    \"\"\"Apply preprocessing to a specific column in a pandas DataFrame\"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Apply preprocessing to the specified column\n",
    "    df[column_name] = df[column_name].apply(preprocess_hindi_text)\n",
    "    \n",
    "    # Save the preprocessed DataFrame back to CSV (optional)\n",
    "    output_file = csv_file.replace('.csv', '_preprocessed.csv')\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c123b206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelizing the code with cuda libraries\n",
    "# Since CUDA is not supporting langdetect module\n",
    "# detect language in cpu and offload the rest of the processing to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbcae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import re\n",
    "from langdetect import detect\n",
    "\n",
    "@cuda.jit\n",
    "def kernel_process(text_array, output_array, text_lengths):\n",
    "    idx = cuda.grid(1)  # Get the thread index\n",
    "    if idx < text_array.shape[0]:  # Check bounds\n",
    "        # Convert input text to string (working with bytes)\n",
    "        text = ''\n",
    "        for i in range(text_lengths[idx]):\n",
    "            text += chr(text_array[idx, i])  # Convert byte to char\n",
    "        \n",
    "        # Step 1: Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Step 2: Remove text in parentheses\n",
    "        text = re.sub(r'\\([^()]*\\)', '', text)\n",
    "        \n",
    "        # Step 3: Remove double quotes\n",
    "        text = text.replace('\"', '')\n",
    "        \n",
    "        # Step 4: Keep only Hindi characters and spaces (Unicode range \\u0900-\\u097F)\n",
    "        text = ''.join(c for c in text if (0x0900 <= ord(c) <= 0x097F) or c.isspace())\n",
    "        \n",
    "        # Step 5: Normalize spaces\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Write result to output array\n",
    "        for i, c in enumerate(text):\n",
    "            if i < out_array.shape[1]:  # Ensure we don't exceed output buffer\n",
    "                out_array[idx, i] = ord(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_hindi_text_gpu(texts):\n",
    "    \"\"\"Preprocess an array of texts on GPU\"\"\"\n",
    "    if not texts:\n",
    "        return []\n",
    "    \n",
    "    # Step 1: Filter Hindi text on CPU (langdetect isn't GPU-friendly)\n",
    "    hindi_texts = []\n",
    "    for text in texts:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            hindi_texts.append(\"\")\n",
    "            continue\n",
    "        segments = text.replace('\\n', ' ').split(' ')\n",
    "        hindi_segments = [seg for seg in segments if seg.strip() and detect(seg) == 'hi']\n",
    "        hindi_texts.append(' '.join(hindi_segments))\n",
    "    \n",
    "    # Step 2: Prepare data for GPU\n",
    "    max_len = max(len(t) for t in hindi_texts) + 1  # Add 1 for safety\n",
    "    text_array = np.zeros((len(hindi_texts), max_len), dtype=np.uint8)\n",
    "    for i, text in enumerate(hindi_texts):\n",
    "        for j, char in enumerate(text):\n",
    "            text_array[i, j] = ord(char)\n",
    "    \n",
    "    # Copy data to GPU\n",
    "    d_text_array = cuda.to_device(text_array)\n",
    "    d_out_array = cuda.device_array((len(hindi_texts), max_len), dtype=np.uint8)\n",
    "    text_lengths = np.array([len(t) for t in hindi_texts], dtype=np.int32)\n",
    "    d_text_lengths = cuda.to_device(text_lengths)\n",
    "    \n",
    "    # Configure CUDA grid and blocks\n",
    "    threads_per_block = 256\n",
    "    blocks_per_grid = (len(hindi_texts) + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    # Launch kernel\n",
    "    preprocess_gpu_kernel[blocks_per_grid, threads_per_block](d_text_array, d_out_array, d_text_lengths)\n",
    "    \n",
    "    # Copy results back to CPU\n",
    "    out_array = d_out_array.copy_to_host()\n",
    "    result = [''.join(chr(c) for c in row if c != 0) for row in out_array]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe_gpu(csv_file, column_name='full_text'):\n",
    "    \"\"\"Apply GPU-accelerated preprocessing to a DataFrame column\"\"\"\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract texts as a list\n",
    "    texts = df[column_name].tolist()\n",
    "    \n",
    "    # Process texts on GPU\n",
    "    preprocessed_texts = preprocess_hindi_text_gpu(texts)\n",
    "    \n",
    "    # Update DataFrame\n",
    "    df[column_name] = preprocessed_texts\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = csv_file.replace('.csv', '_preprocessed_gpu.csv')\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Sarvam.ai)",
   "language": "python",
   "name": "sarvamai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
