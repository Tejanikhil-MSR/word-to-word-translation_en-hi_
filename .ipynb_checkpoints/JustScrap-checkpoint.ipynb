{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833cc764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For language detection\n",
    "#!pip install langdetect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1034e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from langdetect import detect\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6a0ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Language: hi\n"
     ]
    }
   ],
   "source": [
    "detected_lang = detect(\"यह हिंदी भाषा में एक लेख है।\")\n",
    "print(\"Detected Language:\", detected_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b7fb428",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class WikipediaScraper:\n",
    "    def __init__(self, max_depth, max_articles, desired_language, start_url, csv_filename):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_articles = max_articles\n",
    "        self.desired_language = desired_language\n",
    "        self.visited = set()\n",
    "        self.article_count = 0\n",
    "        self.base_url = '/'.join(start_url.split('/')[:3])  # e.g., \"https://hi.wikipedia.org\"\n",
    "        self.csv_filename = csv_filename\n",
    "        \n",
    "        with open(self.csv_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['title', 'url', 'full_text', 'word_count'])\n",
    "            writer.writeheader()\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"Fetch page content with error handling\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_first_line(self, soup):\n",
    "        \"\"\"Extract the first line of meaningful content\"\"\"\n",
    "        if not soup:\n",
    "            return None\n",
    "        content = soup.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            for p in content.find_all('p', recursive=False):\n",
    "                text = p.get_text(strip=True)\n",
    "                if text:\n",
    "                    sentences = text.split('. ')\n",
    "                    return sentences[0] + ('.' if sentences else '')\n",
    "            return content.get_text(strip=True).split('. ')[0] + '.'\n",
    "        return None\n",
    "\n",
    "    def check_language(self, text):\n",
    "        \"\"\"Check if the text is in the desired language\"\"\"\n",
    "        if not text:\n",
    "            return False\n",
    "        try:\n",
    "            detected_lang = detect(text)\n",
    "            return detected_lang == self.desired_language\n",
    "        except Exception as e:\n",
    "            print(f\"Language detection error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def extract_article_info(self, soup, url):\n",
    "        \"\"\"Extract title and full article text (max 500 words), list items min 5 words\"\"\"\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        title = soup.find('h1', id='firstHeading')\n",
    "        if not title:\n",
    "            return None\n",
    "\n",
    "        content = soup.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            full_text = \"\"\n",
    "            word_count = 0\n",
    "            max_words = 500\n",
    "            min_words_for_points = 5\n",
    "\n",
    "            for element in content.find_all(['p', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):\n",
    "                if word_count >= max_words:\n",
    "                    break\n",
    "\n",
    "                if element.get('class') in ['mw-references', 'reflist']:\n",
    "                    continue\n",
    "\n",
    "                if element.name == 'li':\n",
    "                    text = element.get_text(strip=True)\n",
    "                    words = text.split()\n",
    "                    if len(words) < min_words_for_points:\n",
    "                        continue\n",
    "                    parent = element.find_parent(['ul', 'ol'])\n",
    "                    prefix = '- ' if parent and parent.name == 'ul' else '1. '\n",
    "                    text = f\"{prefix}{text}\"\n",
    "                else:\n",
    "                    text = element.get_text(strip=True)\n",
    "\n",
    "                if text:\n",
    "                    words = text.split()\n",
    "                    remaining_words = max_words - word_count\n",
    "                    \n",
    "                    if len(words) <= remaining_words:\n",
    "                        full_text += text + \"\\n\"\n",
    "                        word_count += len(words)\n",
    "                    else:\n",
    "                        truncated_words = words[:remaining_words]\n",
    "                        full_text += \" \".join(truncated_words) + \"\\n\"\n",
    "                        word_count = max_words\n",
    "                        break\n",
    "            \n",
    "            if full_text.strip():\n",
    "                return {\n",
    "                    'title': title.text,\n",
    "                    'url': url,\n",
    "                    'full_text': full_text.strip(),\n",
    "                    'word_count': word_count\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'title': title.text,\n",
    "            'url': url,\n",
    "            'full_text': \"No content found\"\n",
    "        }\n",
    "\n",
    "    def get_wiki_links(self, soup):\n",
    "        \"\"\"Extract all Wikipedia article links and their display text from the page\"\"\"\n",
    "        if not soup:\n",
    "            return []\n",
    "        \n",
    "        links = []\n",
    "        content = soup.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            for a_tag in content.find_all('a', href=True):\n",
    "                href = a_tag['href']\n",
    "                if (href.startswith('/wiki/') and \n",
    "                    not ':' in href and \n",
    "                    not href.startswith('/wiki/Main_Page')):\n",
    "                    full_url = self.base_url + href\n",
    "                    display_text = a_tag.get_text(strip=True) or full_url.split('/')[-1]  # Fallback to URL end if no text\n",
    "                    links.append((display_text, full_url))\n",
    "        return links  # Returns list of (display_text, url) tuples\n",
    "    \n",
    "    def write_to_csv(self, article_info):\n",
    "        \"\"\"Write article info directly to CSV\"\"\"\n",
    "        with open(self.csv_filename, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['title', 'url', 'full_text', 'word_count'])\n",
    "            writer.writerow(article_info)\n",
    "            \n",
    "    def dfs_scrape(self, url, current_depth=0):\n",
    "        \"\"\"Recursive DFS traversal with language checking\"\"\"\n",
    "        if (current_depth >= self.max_depth or \n",
    "            self.article_count >= self.max_articles or \n",
    "            url in self.visited):\n",
    "            return\n",
    "\n",
    "        self.visited.add(url)\n",
    "        \n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            return\n",
    "\n",
    "        # Check language of first line\n",
    "        first_line = self.get_first_line(soup)\n",
    "        if not self.check_language(first_line):\n",
    "            print(f\"Depth {current_depth}: Skipping {url.split('/')[-1]} (Language: {detect(first_line)}, Desired: {self.desired_language})\")\n",
    "            return  # Backtrack if language doesn't match\n",
    "\n",
    "        # Get display text for this URL from the parent page (approximated here)\n",
    "        title = soup.find('h1', id='firstHeading').text if soup.find('h1', id='firstHeading') else url.split('/')[-1]\n",
    "        print(f\"Depth {current_depth}: Scraping {title}\")\n",
    "        article_info = self.extract_article_info(soup, url)\n",
    "        \n",
    "        if article_info:\n",
    "            self.write_to_csv(article_info)\n",
    "            self.article_count += 1\n",
    "\n",
    "        if self.article_count >= self.max_articles:\n",
    "            return\n",
    "\n",
    "        links = self.get_wiki_links(soup)\n",
    "        random.shuffle(links)\n",
    "        \n",
    "        for display_text, next_url in links:\n",
    "            if self.article_count < self.max_articles:\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                self.dfs_scrape(next_url, current_depth + 1)\n",
    "\n",
    "    def scrape(self, start_url=\"https://hi.wikipedia.org/wiki/%E0%A4%B9%E0%A5%88%E0%A4%A6%E0%A4%B0%E0%A4%BE%E0%A4%AC%E0%A4%BE%E0%A4%A6_%E0%A4%95%E0%A5%87_%E0%A4%A8%E0%A4%BF%E0%A4%9C%E0%A4%BC%E0%A4%BE%E0%A4%AE\"):\n",
    "        \"\"\"Start the scraping process\"\"\"\n",
    "        self.dfs_scrape(start_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264483c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth 0: Scraping हैदराबाद के निज़ाम\n",
      "Depth 1: Scraping जीनोम वैली\n",
      "Depth 2: Scraping यदाद्री भुवनगरी ज़िला\n",
      "Depth 3: Scraping हैदराबाद जिला\n",
      "Depth 4: Scraping अदिलाबाद ज़िला\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScraping complete. Results written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscraper\u001b[38;5;241m.\u001b[39mcsv_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m start_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://hi.wikipedia.org/wiki/\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB9\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA5\u001b[39m\u001b[38;5;132;01m%88%\u001b[39;00m\u001b[38;5;124mE0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA6\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB0\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAC\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA6_\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;132;01m%95%\u001b[39;00m\u001b[38;5;124mE0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA5\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m87_\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA8\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBF\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9C\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBC\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m scraper \u001b[38;5;241m=\u001b[39m WikipediaScraper(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, max_articles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, desired_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      4\u001b[0m                            start_url\u001b[38;5;241m=\u001b[39mstart_url, csv_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabc.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScraping complete. Results written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscraper\u001b[38;5;241m.\u001b[39mcsv_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 179\u001b[0m, in \u001b[0;36mWikipediaScraper.scrape\u001b[1;34m(self, start_url)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscrape\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://hi.wikipedia.org/wiki/\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB9\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA5\u001b[39m\u001b[38;5;132;01m%88%\u001b[39;00m\u001b[38;5;124mE0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA6\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB0\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAC\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA6_\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;132;01m%95%\u001b[39;00m\u001b[38;5;124mE0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA5\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m87_\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA8\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBF\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9C\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBC\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    178\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Start the scraping process\"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfs_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 175\u001b[0m, in \u001b[0;36mWikipediaScraper.dfs_scrape\u001b[1;34m(self, url, current_depth)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marticle_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_articles:\n\u001b[0;32m    174\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfs_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 175\u001b[0m, in \u001b[0;36mWikipediaScraper.dfs_scrape\u001b[1;34m(self, url, current_depth)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marticle_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_articles:\n\u001b[0;32m    174\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfs_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: WikipediaScraper.dfs_scrape at line 175 (1 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[9], line 175\u001b[0m, in \u001b[0;36mWikipediaScraper.dfs_scrape\u001b[1;34m(self, url, current_depth)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marticle_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_articles:\n\u001b[0;32m    174\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfs_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 174\u001b[0m, in \u001b[0;36mWikipediaScraper.dfs_scrape\u001b[1;34m(self, url, current_depth)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m display_text, next_url \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marticle_count \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_articles:\n\u001b[1;32m--> 174\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs_scrape(next_url, current_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    start_url = \"https://hi.wikipedia.org/wiki/%E0%A4%B9%E0%A5%88%E0%A4%A6%E0%A4%B0%E0%A4%BE%E0%A4%AC%E0%A4%BE%E0%A4%A6_%E0%A4%95%E0%A5%87_%E0%A4%A8%E0%A4%BF%E0%A4%9C%E0%A4%BC%E0%A4%BE%E0%A4%AE\"\n",
    "    scraper = WikipediaScraper(max_depth=2000, max_articles=2000, desired_language='hi', \n",
    "                               start_url=start_url, csv_filename=\"abc.csv\")\n",
    "    scraper.scrape()\n",
    "    \n",
    "    print(f\"\\nScraping complete. Results written to {scraper.csv_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7745da6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping हैदराबाद के निज़ाम\n",
      "Scraping हैदराबाद हाउस\n",
      "Scraping महाराष्ट्र\n",
      "Scraping बोल्लारम\n",
      "Scraping मूसी नदी\n",
      "Scraping वास्तुकला\n",
      "Scraping निज़ाम-उल-मुल्क आसफजाह\n",
      "Scraping आन्ध्र प्रदेश\n",
      "Scraping हुसैन सागर\n",
      "Scraping बिरला मंदिर, हैदराबाद\n",
      "Scraping गोलकोण्डा\n",
      "Scraping भारत के शहरों की सूची\n",
      "Scraping कोलकाता\n",
      "Scraping निजाम संग्रहालय\n",
      "Scraping चौमोहल्ला पैलेस\n",
      "Scraping निज़ामाबाद जिला\n",
      "Scraping आसफ़ जाही राजवंश\n",
      "Scraping फलकनुमा पैलेस\n",
      "Scraping मीर उस्मान अली ख़ान\n",
      "Scraping हैदराबादी खाना\n",
      "Scraping जीनोम वैली\n",
      "Scraping हैदराबाद जिला\n",
      "Scraping गवर्नमेंट निज़ामिआ जनरल हॉस्पिटल\n",
      "Scraping हैदराबाद विश्वविद्यालय\n",
      "Scraping संस्कृति\n",
      "Scraping तेलंगाना\n",
      "Scraping ग्रेटर हैदराबाद नगर निगम\n",
      "Scraping नई दिल्ली\n",
      "Scraping हैदराबाद\n",
      "Scraping राजशाही विभाग\n",
      "Scraping बेगमपेट विमानक्षेत्र\n",
      "Scraping मुग़ल साम्राज्य\n",
      "Scraping कर्नाटक\n",
      "Scraping हैदराबाद प्रांत\n",
      "Scraping शमशाबाद, आगरा\n",
      "Scraping सिकंदराबाद\n",
      "Scraping अजमेर शरीफ़\n",
      "Scraping कन्नड़\n",
      "Scraping तेलुगू भाषा\n",
      "Scraping राजीव गांधी अंतर्राष्ट्रीय क्रिकेट स्टेडियम\n",
      "Scraping राजीव गाँधी अंतर्राष्ट्रीय विमानक्षेत्र\n",
      "Scraping डेक्कन चार्जर्स\n",
      "Scraping सनराइजर्स हैदराबाद\n",
      "Scraping निजाम कॉलेज\n",
      "Scraping साहित्य\n",
      "Scraping हैदराबाद प्रांत\n",
      "Scraping औरंगज़ेब\n",
      "Scraping निज़ाम का राष्ट्रीय गान\n",
      "Scraping निज़ामाबाद\n",
      "Scraping मक्का मस्जिद\n",
      "Scraping रामोजी फिल्म सिटी\n",
      "Scraping भारतीय प्रौद्योगिकी संस्थान, हैदराबाद\n",
      "Scraping उर्दू भाषा\n",
      "Scraping हैदराबादी बिरयानी\n",
      "Scraping दक्कन का पठार\n",
      "Scraping हैदराबादी हलीम\n",
      "Scraping पुरानी हवेली\n",
      "Scraping रक्षा अनुसंधान एवं विकास संगठन\n",
      "Scraping हैदराबाद मेट्रो\n",
      "Scraping मेड्चल\n",
      "Scraping किंग कोठी पैलेस\n",
      "Scraping कला\n",
      "Scraping हिमायत सागर\n",
      "Scraping निज़ाम्स इंस्टिट्यूट ऑफ़ मेडिकल साइंसस\n",
      "Scraping मराठी भाषा\n",
      "Scraping उस्मान सागर\n",
      "Scraping लाल बहादुर शास्त्री स्टेडियम\n",
      "Scraping जवाहरलाल नेहरू टेक्नोलॉजिकल यूनिवर्सिटी, हैदराबाद\n",
      "Scraping चारमीनार\n",
      "Scraping मिज़ोरम\n",
      "Scraping महाराष्ट्र विधान परिषद\n",
      "Scraping राष्ट्रकूट राजवंश\n",
      "Scraping भोकरदन\n",
      "Scraping वाहन पंजीकरण\n",
      "Scraping मराठी भाषा\n",
      "Scraping २०११\n",
      "Scraping वेबैक मशीन\n",
      "Scraping पुणे\n",
      "Scraping दादरा और नगर हवेली एवं दमन और दीव\n",
      "Scraping नांदेड़\n",
      "Scraping चण्डीगढ़\n",
      "Scraping संस्कृत भाषा\n",
      "Scraping महाराष्ट्र में धर्म\n",
      "Scraping राज्य\n",
      "Scraping अहमदनगर जिला\n",
      "Scraping नागपुर जिला\n",
      "Skipping %E0%A4%9A%E0%A4%BE%E0%A4%B2%E0%A5%81%E0%A4%95%E0%A5%8D%E0%A4%AF (Language: mr, Desired: hi)\n",
      "Language detection error: No features in text.\n",
      "Scraping केन्द्र-शासित प्रदेश\n",
      "Scraping भंडारा जिला\n",
      "Scraping महिला\n"
     ]
    },
    {
     "ename": "LangDetectException",
     "evalue": "No features in text.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLangDetectException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 201\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScraping complete. Results written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscraper\u001b[38;5;241m.\u001b[39mcsv_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 201\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[72], line 197\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m start_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://hi.wikipedia.org/wiki/\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB9\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA5\u001b[39m\u001b[38;5;132;01m%88%\u001b[39;00m\u001b[38;5;124mE0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA6\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mB0\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAC\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA6_\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;132;01m%95%\u001b[39;00m\u001b[38;5;124mE0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA5\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m87_\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA8\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBF\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m9C\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBC\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mBE\u001b[39m\u001b[38;5;132;01m%E\u001b[39;00m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mA4\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mAE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m scraper \u001b[38;5;241m=\u001b[39m WikipediaScraper(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, \n\u001b[0;32m    192\u001b[0m                          max_articles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, \n\u001b[0;32m    193\u001b[0m                          desired_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    194\u001b[0m                          start_url\u001b[38;5;241m=\u001b[39mstart_url, \n\u001b[0;32m    195\u001b[0m                          csv_filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwikipedia_scrape.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m                          max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m--> 197\u001b[0m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mScraping complete. Results written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscraper\u001b[38;5;241m.\u001b[39mcsv_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[72], line 182\u001b[0m, in \u001b[0;36mWikipediaScraper.scrape\u001b[1;34m(self, start_url)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# Process results and add new links to queue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m--> 182\u001b[0m     links \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m     random\u001b[38;5;241m.\u001b[39mshuffle(links)\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, next_url \u001b[38;5;129;01min\u001b[39;00m links:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[1;32mIn[72], line 151\u001b[0m, in \u001b[0;36mWikipediaScraper.process_page\u001b[1;34m(self, url, depth)\u001b[0m\n\u001b[0;32m    149\u001b[0m first_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_first_line(soup)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_language(first_line):\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Language: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetect(first_line)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Desired: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesired_language\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m    154\u001b[0m title \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirstHeading\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mif\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirstHeading\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\site-packages\\langdetect\\detector_factory.py:130\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    128\u001b[0m detector \u001b[38;5;241m=\u001b[39m _factory\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[0;32m    129\u001b[0m detector\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\site-packages\\langdetect\\detector.py:136\u001b[0m, in \u001b[0;36mDetector.detect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Detect language of the target text and return the language name\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    which has the highest probability.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m probabilities:\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m probabilities[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlang\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\site-packages\\langdetect\\detector.py:143\u001b[0m, in \u001b[0;36mDetector.get_probabilities\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_probabilities\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detect_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_probability(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\site-packages\\langdetect\\detector.py:150\u001b[0m, in \u001b[0;36mDetector._detect_block\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m ngrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_ngrams()\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ngrams:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LangDetectException(ErrorCode\u001b[38;5;241m.\u001b[39mCantDetectError, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo features in text.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanglist)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n",
      "\u001b[1;31mLangDetectException\u001b[0m: No features in text."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Queue\n",
    "import threading\n",
    "\n",
    "class WikipediaScraper:\n",
    "    def __init__(self, max_depth, max_articles, desired_language, start_url, csv_filename, max_workers=4):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_articles = max_articles\n",
    "        self.desired_language = desired_language\n",
    "        self.visited = set()\n",
    "        self.article_count = 0\n",
    "        self.base_url = '/'.join(start_url.split('/')[:3])\n",
    "        self.csv_filename = csv_filename\n",
    "        self.max_workers = max_workers\n",
    "        self.lock = threading.Lock()  # For thread-safe CSV writing and counter updates\n",
    "        \n",
    "        # Initialize CSV file with headers\n",
    "        with open(self.csv_filename, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['title', 'url', 'full_text', 'word_count'])\n",
    "            writer.writeheader()\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"Fetch page content with error handling\"\"\"\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_first_line(self, soup):\n",
    "        \"\"\"Extract the first line of meaningful content\"\"\"\n",
    "        if not soup:\n",
    "            return None\n",
    "        content = soup.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            for p in content.find_all('p', recursive=False):\n",
    "                text = p.get_text(strip=True)\n",
    "                if text:\n",
    "                    sentences = text.split('. ')\n",
    "                    return sentences[0] + ('.' if sentences else '')\n",
    "            return content.get_text(strip=True).split('. ')[0] + '.'\n",
    "        return None\n",
    "\n",
    "    def check_language(self, text):\n",
    "        \"\"\"Check if the text is in the desired language\"\"\"\n",
    "        if not text:\n",
    "            return False\n",
    "        try:\n",
    "            detected_lang = detect(text)\n",
    "            return detected_lang == self.desired_language\n",
    "        except Exception as e:\n",
    "            print(f\"Language detection error: {e}\")\n",
    "            return False\n",
    "\n",
    "    def extract_article_info(self, soup, url):\n",
    "        \"\"\"Extract title and full article text (max 500 words), list items min 5 words\"\"\"\n",
    "        if not soup:\n",
    "            return None\n",
    "\n",
    "        title = soup.find('h1', id='firstHeading')\n",
    "        if not title:\n",
    "            return None\n",
    "\n",
    "        content = soup.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            full_text = \"\"\n",
    "            word_count = 0\n",
    "            max_words = 500\n",
    "            min_words_for_points = 5\n",
    "\n",
    "            for element in content.find_all(['p', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):\n",
    "                if word_count >= max_words:\n",
    "                    break\n",
    "                if element.get('class') in ['mw-references', 'reflist']:\n",
    "                    continue\n",
    "                if element.name == 'li':\n",
    "                    text = element.get_text(strip=True)\n",
    "                    words = text.split()\n",
    "                    if len(words) < min_words_for_points:\n",
    "                        continue\n",
    "                    parent = element.find_parent(['ul', 'ol'])\n",
    "                    prefix = '- ' if parent and parent.name == 'ul' else '1. '\n",
    "                    text = f\"{prefix}{text}\"\n",
    "                else:\n",
    "                    text = element.get_text(strip=True)\n",
    "                if text:\n",
    "                    words = text.split()\n",
    "                    remaining_words = max_words - word_count\n",
    "                    if len(words) <= remaining_words:\n",
    "                        full_text += text + \"\\n\"\n",
    "                        word_count += len(words)\n",
    "                    else:\n",
    "                        truncated_words = words[:remaining_words]\n",
    "                        full_text += \" \".join(truncated_words) + \"\\n\"\n",
    "                        word_count = max_words\n",
    "                        break\n",
    "            \n",
    "            if full_text.strip():\n",
    "                return {'title': title.text, 'url': url, 'full_text': full_text.strip(), 'word_count': word_count}\n",
    "        \n",
    "        return {'title': title.text, 'url': url, 'full_text': \"No content found\", 'word_count': 0}\n",
    "\n",
    "    def write_to_csv(self, article_info):\n",
    "        \"\"\"Write article info directly to CSV with thread safety\"\"\"\n",
    "        with self.lock:  # Ensure thread-safe writing\n",
    "            with open(self.csv_filename, 'a', newline='', encoding='utf-8') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=['title', 'url', 'full_text', 'word_count'])\n",
    "                writer.writerow(article_info)\n",
    "\n",
    "    def get_wiki_links(self, soup):\n",
    "        \"\"\"Extract all Wikipedia article links and their display text from the page\"\"\"\n",
    "        if not soup:\n",
    "            return []\n",
    "        links = []\n",
    "        content = soup.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            for a_tag in content.find_all('a', href=True):\n",
    "                href = a_tag['href']\n",
    "                if (href.startswith('/wiki/') and \n",
    "                    not ':' in href and \n",
    "                    not href.startswith('/wiki/Main_Page')):\n",
    "                    full_url = self.base_url + href\n",
    "                    display_text = a_tag.get_text(strip=True) or full_url.split('/')[-1]\n",
    "                    links.append((display_text, full_url))\n",
    "        return links\n",
    "\n",
    "    def process_page(self, url, depth):\n",
    "        \"\"\"Process a single page and return links if successful\"\"\"\n",
    "        with self.lock:\n",
    "            if (depth >= self.max_depth or \n",
    "                self.article_count >= self.max_articles or \n",
    "                url in self.visited):\n",
    "                return []\n",
    "            self.visited.add(url)\n",
    "\n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            return []\n",
    "\n",
    "        first_line = self.get_first_line(soup)\n",
    "        if not self.check_language(first_line):\n",
    "            print(f\"Skipping {url.split('/')[-1]} (Language: {detect(first_line)}, Desired: {self.desired_language})\")\n",
    "            return []\n",
    "\n",
    "        title = soup.find('h1', id='firstHeading').text if soup.find('h1', id='firstHeading') else url.split('/')[-1]\n",
    "        print(f\"Scraping {title}\")\n",
    "        article_info = self.extract_article_info(soup, url)\n",
    "        \n",
    "        if article_info:\n",
    "            self.write_to_csv(article_info)\n",
    "            with self.lock:\n",
    "                self.article_count += 1\n",
    "        \n",
    "        return self.get_wiki_links(soup) if self.article_count < self.max_articles else []\n",
    "\n",
    "    def scrape(self, start_url=\"https://hi.wikipedia.org/wiki/%E0%A4%B9%E0%A5%88%E0%A4%A6%E0%A4%B0%E0%A4%BE%E0%A4%AC%E0%A4%BE%E0%A4%A6_%E0%A4%95%E0%A5%87_%E0%A4%A8%E0%A4%BF%E0%A4%9C%E0%A4%BC%E0%A4%BE%E0%A4%AE\"):\n",
    "        \"\"\"Start the scraping process with parallel workers\"\"\"\n",
    "        queue = Queue()\n",
    "        queue.put((start_url, 0))  # (url, depth)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            while not queue.empty() and self.article_count < self.max_articles:\n",
    "                futures = []\n",
    "                # Grab up to max_workers items from queue\n",
    "                for _ in range(min(self.max_workers, queue.qsize())):\n",
    "                    if queue.empty():\n",
    "                        break\n",
    "                    url, depth = queue.get()\n",
    "                    futures.append(executor.submit(self.process_page, url, depth))\n",
    "\n",
    "                # Process results and add new links to queue\n",
    "                for future in futures:\n",
    "                    links = future.result()\n",
    "                    random.shuffle(links)\n",
    "                    for _, next_url in links:\n",
    "                        if self.article_count < self.max_articles and next_url not in self.visited:\n",
    "                            queue.put((next_url, depth + 1))\n",
    "                    time.sleep(random.uniform(1, 3))  # Polite delay between requests\n",
    "\n",
    "def main():\n",
    "    start_url = \"https://hi.wikipedia.org/wiki/%E0%A4%B9%E0%A5%88%E0%A4%A6%E0%A4%B0%E0%A4%BE%E0%A4%AC%E0%A4%BE%E0%A4%A6_%E0%A4%95%E0%A5%87_%E0%A4%A8%E0%A4%BF%E0%A4%9C%E0%A4%BC%E0%A4%BE%E0%A4%AE\"\n",
    "    scraper = WikipediaScraper(max_depth=2000, \n",
    "                             max_articles=2000, \n",
    "                             desired_language='hi', \n",
    "                             start_url=start_url, \n",
    "                             csv_filename=\"wikipedia_scrape.csv\",\n",
    "                             max_workers=4)\n",
    "    scraper.scrape()\n",
    "    print(f\"\\nScraping complete. Results written to {scraper.csv_filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fba86efe",
   "metadata": {},
   "outputs": [
    {
     "ename": "LangDetectException",
     "evalue": "No features in text.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLangDetectException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m detect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m detect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\site-packages\\langdetect\\detector_factory.py:130\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    128\u001b[0m detector \u001b[38;5;241m=\u001b[39m _factory\u001b[38;5;241m.\u001b[39mcreate()\n\u001b[0;32m    129\u001b[0m detector\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\site-packages\\langdetect\\detector.py:136\u001b[0m, in \u001b[0;36mDetector.detect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Detect language of the target text and return the language name\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    which has the highest probability.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m probabilities:\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m probabilities[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlang\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\site-packages\\langdetect\\detector.py:143\u001b[0m, in \u001b[0;36mDetector.get_probabilities\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_probabilities\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detect_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_probability(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sarvamai\\lib\\site-packages\\langdetect\\detector.py:150\u001b[0m, in \u001b[0;36mDetector._detect_block\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    148\u001b[0m ngrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_ngrams()\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ngrams:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LangDetectException(ErrorCode\u001b[38;5;241m.\u001b[39mCantDetectError, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo features in text.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangprob \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanglist)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n",
      "\u001b[1;31mLangDetectException\u001b[0m: No features in text."
     ]
    }
   ],
   "source": [
    "detect('.')\n",
    "detect(' ')\n",
    "detect('5')\n",
    "detect('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fdc4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from langdetect import detect\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class scraper:\n",
    "    def __init__(self, max_recurse, num_articles, max_article_length, desired_lang, root_url, csv_path):\n",
    "        self.recursion_depth = max_recurse\n",
    "        self.num_articles = num_articles\n",
    "        self.desired_lang = desired_lang\n",
    "        self.max_length = max_article_length\n",
    "        self.seed_url = root_url\n",
    "        self.num_reads = 0\n",
    "        self.sink = csv_path\n",
    "        self.csv_filename = csv_path\n",
    "        self.base_url = '/'.join(root_url.split('/')[:3])\n",
    "        self.visited = set()\n",
    "\n",
    "        with open(self.sink, 'w', newline= '', encoding='utf-8') as f:\n",
    "            file = csv.DictWriter(f, [\"url\",\"content\",\"word_count\"])\n",
    "            file.writeheader()\n",
    "\n",
    "    def get_response(self, url):\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def in_desired_lang(self, text):\n",
    "        print(text)\n",
    "        if(len(text)>=2): # ensuring text is not a special character\n",
    "            try:\n",
    "                detected_lang = detect(text)\n",
    "                return detected_lang == self.desired_lang\n",
    "            except Exception as e:\n",
    "                print(f\"Language detection error: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(text)\n",
    "            print(f\"Error while detecting the language\")\n",
    "            return False\n",
    "\n",
    "    def get_title(self, page):\n",
    "        if not page:\n",
    "            return None\n",
    "        \n",
    "        title = page.find('h1', id=\"firstHeading\")\n",
    "\n",
    "        if not title:\n",
    "            content = page.find('div', class_='mw-parser-output')\n",
    "            if content:\n",
    "                for p in content.find_all('p', recursive=False):\n",
    "                    text = p.get_text(strip=True)\n",
    "                    if text:\n",
    "                        sentences = text.split('. ')\n",
    "                        return sentences[0] + ('.' if sentences else ' ')\n",
    "                return content.get_text(strip=True).split('. ')[0] + '.'\n",
    "            \n",
    "            print(\"found an invalid page\")\n",
    "            return None\n",
    "        \n",
    "        return title.text\n",
    "\n",
    "    def extract_info(self, page, url):\n",
    "\n",
    "        if not page:\n",
    "            return None\n",
    "        \n",
    "        content = page.find('div', class_='mw-parser-output')\n",
    "\n",
    "        if content:\n",
    "            full_text = \"\"\n",
    "            word_count = 0\n",
    "            min_words_for_points = 5 # ensuring that special characters (or) single characters are not read\n",
    "            for element in content.find_all(['p', 'h2', 'h3', 'h4', 'h5', 'h6', 'li']):\n",
    "                if word_count >= self.max_length:\n",
    "                    break\n",
    "\n",
    "                else:\n",
    "                    if element.get('class') in ['mw-references', 'reflist']:\n",
    "                        continue\n",
    "\n",
    "                    if element.name == 'li':\n",
    "                        text = element.get_text(strip=True)\n",
    "                        words = text.split()\n",
    "                        if len(words) < min_words_for_points:\n",
    "                            continue\n",
    "                        parent = element.find_parent(['ul', 'ol'])\n",
    "                        prefix = '- ' if parent and parent.name == 'ul' else '1. '\n",
    "                        text = f\"{prefix}{text}\"\n",
    "                    else:\n",
    "                        text = element.get_text(strip=True)\n",
    "\n",
    "                    if text:\n",
    "                        words = text.split()\n",
    "                        remaining_words = self.max_length - word_count\n",
    "                        \n",
    "                        if len(words) <= remaining_words:\n",
    "                            full_text += text + \"\\n\"\n",
    "                            word_count += len(words)\n",
    "                        else:\n",
    "                            truncated_words = words[:remaining_words]\n",
    "                            full_text += \" \".join(truncated_words) + \"\\n\"\n",
    "                            word_count = self.max_length\n",
    "                            break\n",
    "\n",
    "            if full_text.strip():\n",
    "                return { 'url': url, 'content': full_text.strip(), 'word_count': word_count }\n",
    "        \n",
    "        return {'url': url, 'content': \"no content found\", 'word_count': word_count}\n",
    "\n",
    "    def get_links(self, page):\n",
    "        if not page:\n",
    "            return []\n",
    "        \n",
    "        links = []\n",
    "        content = page.find('div', class_='mw-parser-output')\n",
    "        if content:\n",
    "            for a_tag in content.find_all('a', href=True):\n",
    "                href = a_tag['href']\n",
    "                if (href.startswith('/wiki/') and not ':' in href and not href.startswith('/wiki/Main_Page')):\n",
    "                    full_url = self.base_url + href\n",
    "                    display_text = a_tag.get_text(strip=True) or full_url.split('/')[-1]\n",
    "                    links.append((display_text, full_url))\n",
    "\n",
    "        return links \n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # Step 1: Split text into sentences/segments (handling newlines and points)\n",
    "        segments = text.replace('\\n', ' ').split(' ')\n",
    "        \n",
    "        # Step 2: Keep only Hindi text\n",
    "        hindi_text = []\n",
    "        for segment in segments:\n",
    "            if segment.strip():\n",
    "                try:\n",
    "                    if detect(segment) == 'hi':\n",
    "                        hindi_text.append(segment)\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # Rejoin segments into a single string\n",
    "        text = ' '.join(hindi_text)\n",
    "        \n",
    "        # Step 3: Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Step 4: Remove text in parentheses\n",
    "        text = re.sub(r'\\([^()]*\\)', '', text)\n",
    "        \n",
    "        # Step 5: Remove double quotes\n",
    "        text = text.replace('\"', '')\n",
    "        \n",
    "        # Step 6: Remove special characters (keep Hindi characters and spaces)\n",
    "        # Hindi Unicode range: \\u0900-\\u097F\n",
    "        text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n",
    "        \n",
    "        # Step 7: Remove extra spaces and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def write_to_csv(self, info):\n",
    "        with open(self.csv_filename, 'a', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['url', 'content', 'word_count'])\n",
    "            writer.writerow(info)\n",
    "\n",
    "    def DFS(self, url, current_depth=0):\n",
    "        if((url in self.visited) or (self.num_reads >= self.num_articles)):\n",
    "            return\n",
    "        \n",
    "        self.visited.add(url)\n",
    "\n",
    "        page = self.get_response(url)\n",
    "        if not page:\n",
    "            return\n",
    "        \n",
    "        title = self.get_title(page)\n",
    "        if not title:\n",
    "            return\n",
    "        \n",
    "        lang = self.in_desired_lang(title)\n",
    "        if(not lang):\n",
    "            print(f\"Skipping the url, Language detected is different from desired.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Depth {current_depth}: Scraping {title}\")\n",
    "        article_info = self.extract_info(page, url)\n",
    "\n",
    "        if article_info:\n",
    "            preprocessed_text = self.preprocess_text(article_info[\"content\"])\n",
    "            self.write_to_csv({\"url\":article_info[\"url\"], \"content\":preprocessed_text, \"word_count\": article_info[\"word_count\"]})\n",
    "            self.num_reads += 1\n",
    "\n",
    "        if self.num_reads >= self.num_articles:\n",
    "            return\n",
    "        \n",
    "        links = self.get_links(page)\n",
    "        \n",
    "        if current_depth < self.recursion_depth:  \n",
    "            for display_text, next_url in links:\n",
    "                if self.num_reads < self.num_articles:\n",
    "                    print(f\"Scraping {display_text}\")\n",
    "                    self.DFS(next_url, current_depth + 1)  # Go deeper\n",
    "        \n",
    "    def fire(self):\n",
    "        self.DFS(self.seed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6b96e832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "हैदराबाद के निज़ाम\n",
      "Depth 0: Scraping हैदराबाद के निज़ाम\n",
      "Scraping क़मरुद्दीन खान\n",
      "निज़ाम-उल-मुल्क आसफजाह\n",
      "Depth 1: Scraping निज़ाम-उल-मुल्क आसफजाह\n",
      "Saving to scraper.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    start_url = \"https://hi.wikipedia.org/wiki/%E0%A4%B9%E0%A5%88%E0%A4%A6%E0%A4%B0%E0%A4%BE%E0%A4%AC%E0%A4%BE%E0%A4%A6_%E0%A4%95%E0%A5%87_%E0%A4%A8%E0%A4%BF%E0%A4%9C%E0%A4%BC%E0%A4%BE%E0%A4%AE\"\n",
    "    my_scraper = scraper(max_recurse=4, num_articles=2, max_article_length=500, desired_lang=\"hi\", root_url=start_url, csv_path=\"scraper.csv\")\n",
    "    my_scraper.fire()\n",
    "    print(f\"Saving to {my_scraper.csv_filename}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bd25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Sarvam.ai)",
   "language": "python",
   "name": "sarvamai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
